{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c7d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu Q-LoRA fine-tuning BGE-M3...\n",
      "Loading tokenizer...\n",
      "Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 353\u001b[39m\n\u001b[32m    350\u001b[39m np.random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# Ch·∫°y training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    356\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müéâ Q-LoRA Fine-tuning ho√†n th√†nh!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 225\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ B·∫Øt ƒë·∫ßu Q-LoRA fine-tuning BGE-M3...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# Load model v√† tokenizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m model, tokenizer = \u001b[43mload_model_with_qlora\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# T·∫°o d·ªØ li·ªáu\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä T·∫°o training data...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mload_model_with_qlora\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m     51\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model with 4-bit quantization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_bnb_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# T·ª± ƒë·ªông ph√¢n b·ªï tr√™n 2 GPU\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreparing model for k-bit training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m model = prepare_model_for_kbit_training(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\transformers\\modeling_utils.py:4228\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4225\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4228\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4235\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4236\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:76\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     73\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# Q-LoRA Fine-tuning BGE-M3 tr√™n Kaggle v·ªõi 2x T4 GPU\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# C·∫•u h√¨nh 4-bit quantization cho Q-LoRA\n",
    "def get_bnb_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                    # K√≠ch ho·∫°t 4-bit loading\n",
    "        bnb_4bit_use_double_quant=True,       # Double quantization\n",
    "        bnb_4bit_quant_type=\"nf4\",           # NormalFloat4 quantization\n",
    "        bnb_4bit_compute_dtype=torch.float16, # Compute dtype\n",
    "    )\n",
    "\n",
    "# C·∫•u h√¨nh LoRA\n",
    "def get_lora_config():\n",
    "    return LoraConfig(\n",
    "        r=16,                           # Rank c·ªßa decomposition\n",
    "        lora_alpha=32,                  # Scaling factor (th∆∞·ªùng = 2*r)\n",
    "        target_modules=[                # Modules c·∫ßn √°p d·ª•ng LoRA\n",
    "            \"query\", \"key\", \"value\", \"dense\"\n",
    "        ],\n",
    "        lora_dropout=0.1,              # Dropout rate\n",
    "        bias=\"none\",                   # Kh√¥ng train bias\n",
    "        task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "\n",
    "# Load model v·ªõi Q-LoRA\n",
    "def load_model_with_qlora(model_name=\"BAAI/bge-m3\"):\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    print(\"Loading model with 4-bit quantization...\")\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=get_bnb_config(),\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",              # T·ª± ƒë·ªông ph√¢n b·ªï tr√™n 2 GPU\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"Preparing model for k-bit training...\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    print(\"Adding LoRA adapters...\")\n",
    "    model = get_peft_model(model, get_lora_config())\n",
    "    \n",
    "    # In th√¥ng tin v·ªÅ trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Custom Dataset class cho sentence similarity\n",
    "class SentencePairDataset:\n",
    "    def __init__(self, sentences1, sentences2, labels, tokenizer, max_length=512):\n",
    "        self.sentences1 = sentences1\n",
    "        self.sentences2 = sentences2\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize c√¢u th·ª© nh·∫•t\n",
    "        encoding1 = self.tokenizer(\n",
    "            self.sentences1[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize c√¢u th·ª© hai\n",
    "        encoding2 = self.tokenizer(\n",
    "            self.sentences2[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids_1': encoding1['input_ids'].squeeze(),\n",
    "            'attention_mask_1': encoding1['attention_mask'].squeeze(),\n",
    "            'input_ids_2': encoding2['input_ids'].squeeze(),\n",
    "            'attention_mask_2': encoding2['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Custom Trainer cho sentence similarity\n",
    "class SentenceSimilarityTrainer(Trainer):\n",
    "    def __init__(self, model, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "        self.model = model\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract inputs\n",
    "        input_ids_1 = inputs['input_ids_1']\n",
    "        attention_mask_1 = inputs['attention_mask_1']\n",
    "        input_ids_2 = inputs['input_ids_2']\n",
    "        attention_mask_2 = inputs['attention_mask_2']\n",
    "        labels = inputs['labels']\n",
    "        \n",
    "        # Get embeddings cho c√¢u th·ª© nh·∫•t\n",
    "        outputs1 = model(input_ids=input_ids_1, attention_mask=attention_mask_1)\n",
    "        embeddings1 = self.mean_pooling(outputs1.last_hidden_state, attention_mask_1)\n",
    "        \n",
    "        # Get embeddings cho c√¢u th·ª© hai  \n",
    "        outputs2 = model(input_ids=input_ids_2, attention_mask=attention_mask_2)\n",
    "        embeddings2 = self.mean_pooling(outputs2.last_hidden_state, attention_mask_2)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings1 = F.normalize(embeddings1, p=2, dim=1)\n",
    "        embeddings2 = F.normalize(embeddings2, p=2, dim=1)\n",
    "        \n",
    "        # T√≠nh cosine similarity\n",
    "        cosine_sim = F.cosine_similarity(embeddings1, embeddings2, dim=1)\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = F.mse_loss(cosine_sim, labels)\n",
    "        \n",
    "        return (loss, {'cosine_sim': cosine_sim}) if return_outputs else loss\n",
    "    \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu m·∫´u cho training\n",
    "def create_sample_data():\n",
    "    # D·ªØ li·ªáu m·∫´u - trong th·ª±c t·∫ø b·∫°n s·∫Ω load t·ª´ file\n",
    "    sentences1 = [\n",
    "        \"The cat is sleeping on the sofa.\",\n",
    "        \"I love programming in Python.\",\n",
    "        \"The weather is nice today.\",\n",
    "        \"Machine learning is fascinating.\",\n",
    "        \"The book is on the table.\"\n",
    "    ] * 100  # Nh√¢n l√™n ƒë·ªÉ c√≥ ƒë·ªß data\n",
    "    \n",
    "    sentences2 = [\n",
    "        \"A cat is resting on the couch.\",\n",
    "        \"Python programming is my favorite.\",\n",
    "        \"Today has beautiful weather.\",\n",
    "        \"AI and ML are very interesting.\",\n",
    "        \"There's a book on the desk.\"\n",
    "    ] * 100\n",
    "    \n",
    "    # Labels t·ª´ 0-1 (similarity scores)\n",
    "    labels = [0.8, 0.9, 0.85, 0.75, 0.7] * 100\n",
    "    \n",
    "    return sentences1, sentences2, labels\n",
    "\n",
    "# H√†m ƒë√°nh gi√° model\n",
    "def evaluate_model(model, tokenizer, test_sentences1, test_sentences2, test_labels):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_sentences1)):\n",
    "            # Tokenize\n",
    "            encoding1 = tokenizer(test_sentences1[i], return_tensors='pt', \n",
    "                                truncation=True, padding=True, max_length=512)\n",
    "            encoding2 = tokenizer(test_sentences2[i], return_tensors='pt',\n",
    "                                truncation=True, padding=True, max_length=512)\n",
    "            \n",
    "            # Move to device\n",
    "            encoding1 = {k: v.to(model.device) for k, v in encoding1.items()}\n",
    "            encoding2 = {k: v.to(model.device) for k, v in encoding2.items()}\n",
    "            \n",
    "            # Get embeddings\n",
    "            outputs1 = model(**encoding1)\n",
    "            outputs2 = model(**encoding2)\n",
    "            \n",
    "            # Mean pooling\n",
    "            def mean_pooling(token_embeddings, attention_mask):\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                return sum_embeddings / sum_mask\n",
    "            \n",
    "            emb1 = mean_pooling(outputs1.last_hidden_state, encoding1['attention_mask'])\n",
    "            emb2 = mean_pooling(outputs2.last_hidden_state, encoding2['attention_mask'])\n",
    "            \n",
    "            # Normalize v√† t√≠nh cosine similarity\n",
    "            emb1 = F.normalize(emb1, p=2, dim=1)\n",
    "            emb2 = F.normalize(emb2, p=2, dim=1)\n",
    "            cosine_sim = F.cosine_similarity(emb1, emb2, dim=1)\n",
    "            \n",
    "            predictions.append(cosine_sim.cpu().item())\n",
    "    \n",
    "    # T√≠nh correlation v·ªõi ground truth\n",
    "    correlation = np.corrcoef(predictions, test_labels)[0, 1]\n",
    "    mse = np.mean((np.array(predictions) - np.array(test_labels)) ** 2)\n",
    "    \n",
    "    return correlation, mse, predictions\n",
    "\n",
    "# Main training function\n",
    "def main():\n",
    "    print(\"üöÄ B·∫Øt ƒë·∫ßu Q-LoRA fine-tuning BGE-M3...\")\n",
    "    \n",
    "    # Load model v√† tokenizer\n",
    "    model, tokenizer = load_model_with_qlora()\n",
    "    \n",
    "    # T·∫°o d·ªØ li·ªáu\n",
    "    print(\"üìä T·∫°o training data...\")\n",
    "    sentences1, sentences2, labels = create_sample_data()\n",
    "    \n",
    "    # Split train/test\n",
    "    split_idx = int(0.8 * len(sentences1))\n",
    "    train_s1, test_s1 = sentences1[:split_idx], sentences1[split_idx:]\n",
    "    train_s2, test_s2 = sentences2[:split_idx], sentences2[split_idx:]\n",
    "    train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "    \n",
    "    # T·∫°o dataset\n",
    "    train_dataset = SentencePairDataset(train_s1, train_s2, train_labels, tokenizer)\n",
    "    \n",
    "    # Training arguments - t·ªëi ∆∞u cho 2x T4 GPU\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,      # Nh·ªè do memory h·∫°n ch·∫ø\n",
    "        gradient_accumulation_steps=4,       # TƒÉng effective batch size\n",
    "        warmup_steps=100,\n",
    "        learning_rate=2e-4,                 # Learning rate cao h∆°n cho LoRA  \n",
    "        fp16=True,                          # Mixed precision training\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"no\",           # T·∫Øt eval ƒë·ªÉ ti·∫øt ki·ªám time\n",
    "        dataloader_num_workers=2,           # Parallel data loading\n",
    "        remove_unused_columns=False,        # Gi·ªØ t·∫•t c·∫£ columns\n",
    "        report_to=None,                     # T·∫Øt wandb/tensorboard\n",
    "    )\n",
    "    \n",
    "    # T·∫°o trainer\n",
    "    trainer = SentenceSimilarityTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # B·∫Øt ƒë·∫ßu training\n",
    "    print(\"üî• B·∫Øt ƒë·∫ßu training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    print(\"üíæ Saving model...\")\n",
    "    trainer.save_model(\"./bge-m3-qlora-finetuned\")\n",
    "    \n",
    "    # ƒê√°nh gi√° model\n",
    "    print(\"üìà Evaluating model...\")\n",
    "    correlation, mse, predictions = evaluate_model(model, tokenizer, test_s1, test_s2, test_labels)\n",
    "    \n",
    "    print(f\"‚úÖ Training ho√†n th√†nh!\")\n",
    "    print(f\"üìä Correlation v·ªõi ground truth: {correlation:.4f}\")\n",
    "    print(f\"üìä MSE: {mse:.4f}\")\n",
    "    \n",
    "    # In m·ªôt v√†i v√≠ d·ª• predictions\n",
    "    print(\"\\nüîç M·ªôt v√†i v√≠ d·ª• predictions:\")\n",
    "    for i in range(min(5, len(test_s1))):\n",
    "        print(f\"C√¢u 1: {test_s1[i]}\")\n",
    "        print(f\"C√¢u 2: {test_s2[i]}\")\n",
    "        print(f\"Ground truth: {test_labels[i]:.3f}\")\n",
    "        print(f\"Prediction: {predictions[i]:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# H√†m ƒë·ªÉ load model ƒë√£ fine-tune v√† s·ª≠ d·ª•ng\n",
    "def load_finetuned_model(model_path=\"./bge-m3-qlora-finetuned\"):\n",
    "    print(\"Loading fine-tuned model...\")\n",
    "    \n",
    "    # Load base model v·ªõi quantization\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        \"BAAI/bge-m3\",\n",
    "        quantization_config=get_bnb_config(),\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load LoRA weights\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# H√†m ƒë·ªÉ encode text th√†nh embeddings\n",
    "def encode_text(model, tokenizer, texts, batch_size=8):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoded = {k: v.to(model.device) for k, v in encoded.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            \n",
    "            # Mean pooling\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "            # Normalize\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            all_embeddings.extend(embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds cho reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Ch·∫°y training\n",
    "    main()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Q-LoRA Fine-tuning ho√†n th√†nh!\")\n",
    "    print(\"üìÅ Model ƒë√£ ƒë∆∞·ª£c save t·∫°i: ./bge-m3-qlora-finetuned\")\n",
    "    print(\"üîß ƒê·ªÉ s·ª≠ d·ª•ng model: model, tokenizer = load_finetuned_model()\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Linhtinh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
