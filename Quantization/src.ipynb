{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8459d54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/hojjatk/mnist-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22.0M/22.0M [00:15<00:00, 1.46MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Hokta\\.cache\\kagglehub\\datasets\\hojjatk\\mnist-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493143c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 0.572\n",
      "[1, 200] loss: 0.155\n",
      "[1, 300] loss: 0.107\n",
      "[1, 400] loss: 0.099\n",
      "[1, 500] loss: 0.084\n",
      "[1, 600] loss: 0.078\n",
      "[1, 700] loss: 0.060\n",
      "[1, 800] loss: 0.065\n",
      "[1, 900] loss: 0.063\n",
      "[2, 100] loss: 0.040\n",
      "[2, 200] loss: 0.047\n",
      "[2, 300] loss: 0.040\n",
      "[2, 400] loss: 0.044\n",
      "[2, 500] loss: 0.042\n",
      "[2, 600] loss: 0.047\n",
      "[2, 700] loss: 0.038\n",
      "[2, 800] loss: 0.044\n",
      "[2, 900] loss: 0.041\n",
      "[3, 100] loss: 0.027\n",
      "[3, 200] loss: 0.033\n",
      "[3, 300] loss: 0.031\n",
      "[3, 400] loss: 0.028\n",
      "[3, 500] loss: 0.028\n",
      "[3, 600] loss: 0.027\n",
      "[3, 700] loss: 0.030\n",
      "[3, 800] loss: 0.029\n",
      "[3, 900] loss: 0.037\n",
      "[4, 100] loss: 0.015\n",
      "[4, 200] loss: 0.023\n",
      "[4, 300] loss: 0.025\n",
      "[4, 400] loss: 0.020\n",
      "[4, 500] loss: 0.027\n",
      "[4, 600] loss: 0.018\n",
      "[4, 700] loss: 0.023\n",
      "[4, 800] loss: 0.028\n",
      "[4, 900] loss: 0.027\n",
      "[5, 100] loss: 0.010\n",
      "[5, 200] loss: 0.013\n",
      "[5, 300] loss: 0.014\n",
      "[5, 400] loss: 0.017\n",
      "[5, 500] loss: 0.014\n",
      "[5, 600] loss: 0.012\n",
      "[5, 700] loss: 0.021\n",
      "[5, 800] loss: 0.023\n",
      "[5, 900] loss: 0.020\n",
      "Huấn luyện hoàn tất!\n",
      "Model FP32:\n",
      "Accuracy: 98.92%\n",
      "Inference time: 4.4104 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:2044 [kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cudnn\\Conv.cpp:386 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:95 [backend fallback]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocastXPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:504 [backend fallback]\nAutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPipeline quantization hoàn tất!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 195\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    192\u001b[39m quantized_model = quantize_model(model, calib_loader)\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# Đánh giá mô hình quantized\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m int8_metrics = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mINT8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# So sánh kích thước mô hình\u001b[39;00m\n\u001b[32m    198\u001b[39m compare_model_sizes(model, quantized_model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_loader, name)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m         _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n\u001b[32m     89\u001b[39m         total += labels.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mSimpleCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     26\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool1(x)\n\u001b[32m     27\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu2(\u001b[38;5;28mself\u001b[39m.conv2(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:152\u001b[39m, in \u001b[36mConvReLU2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    148\u001b[39m     _reversed_padding_repeated_twice = _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m.padding)\n\u001b[32m    149\u001b[39m     \u001b[38;5;28minput\u001b[39m = F.pad(\n\u001b[32m    150\u001b[39m         \u001b[38;5;28minput\u001b[39m, _reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m    151\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d_relu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_point\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\torch\\_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:2044 [kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cudnn\\Conv.cpp:386 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:95 [backend fallback]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocastXPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:504 [backend fallback]\nAutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Định nghĩa mô hình CNN đơn giản\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. Chuẩn bị dữ liệu\n",
    "def load_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "    \n",
    "    # Tập dữ liệu nhỏ cho calibration\n",
    "    calib_sampler = torch.utils.data.RandomSampler(train_dataset, replacement=True, num_samples=100)\n",
    "    calib_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=32, num_workers=0, sampler=calib_sampler\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, calib_loader\n",
    "\n",
    "# 3. Huấn luyện mô hình\n",
    "def train_model(model, train_loader, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % 100 == 99:\n",
    "                print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    print('Huấn luyện hoàn tất!')\n",
    "    return model\n",
    "\n",
    "# 4. Đánh giá mô hình\n",
    "def evaluate_model(model, test_loader, name=\"FP32\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Model {name}:')\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Inference time: {inference_time:.4f} seconds')\n",
    "    \n",
    "    return accuracy, inference_time\n",
    "\n",
    "# 5. Quantization của mô hình\n",
    "def prepare_for_quantization(model):\n",
    "    # Thêm các quan sát giá trị (observers) cho quantization\n",
    "    model_fused = torch.quantization.fuse_modules(model, [['conv1', 'relu1'], \n",
    "                                                         ['conv2', 'relu2'], \n",
    "                                                         ['fc1', 'relu3']])\n",
    "    return model_fused\n",
    "\n",
    "def quantize_model(model, calib_loader):\n",
    "    # Chuẩn bị mô hình\n",
    "    model_fused = prepare_for_quantization(model)\n",
    "    \n",
    "    # Cấu hình quantization\n",
    "    model_fused.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(model_fused, inplace=True)\n",
    "    \n",
    "    # Calibration\n",
    "    model_fused.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in calib_loader:\n",
    "            model_fused(inputs)\n",
    "    \n",
    "    # Chuyển đổi sang mô hình quantized\n",
    "    model_quantized = torch.quantization.convert(model_fused, inplace=True)\n",
    "    \n",
    "    return model_quantized\n",
    "\n",
    "# 6. So sánh kích thước mô hình\n",
    "def compare_model_sizes(fp32_model, int8_model):\n",
    "    import os\n",
    "    def get_model_size(model):\n",
    "        torch.save(model.state_dict(), \"temp.p\")\n",
    "        size = os.path.getsize(\"temp.p\") / 1e6  # MB\n",
    "        os.remove('temp.p')\n",
    "        return size\n",
    "    \n",
    "    fp32_size = get_model_size(fp32_model)\n",
    "    int8_size = get_model_size(int8_model)\n",
    "    \n",
    "    print(f\"FP32 Model Size: {fp32_size:.2f} MB\")\n",
    "    print(f\"INT8 Model Size: {int8_size:.2f} MB\")\n",
    "    print(f\"Compression ratio: {fp32_size/int8_size:.2f}x\")\n",
    "    \n",
    "    return fp32_size, int8_size\n",
    "\n",
    "# 7. Hiển thị kết quả\n",
    "def plot_comparison(fp32_metrics, int8_metrics):\n",
    "    labels = ['FP32', 'INT8']\n",
    "    accuracy = [fp32_metrics[0], int8_metrics[0]]\n",
    "    inference_time = [fp32_metrics[1], int8_metrics[1]]\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, accuracy, width, label='Accuracy (%)', color='blue')\n",
    "    ax1.set_ylabel('Accuracy (%)', color='blue')\n",
    "    ax1.set_ylim([0, 100])\n",
    "    \n",
    "    bars2 = ax2.bar(x + width/2, inference_time, width, label='Inference Time (s)', color='red')\n",
    "    ax2.set_ylabel('Inference Time (s)', color='red')\n",
    "    \n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.title('So sánh mô hình FP32 và INT8 Quantized')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('quantization_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "# 8. Hàm chính\n",
    "def main():\n",
    "    # Tải dữ liệu\n",
    "    train_loader, test_loader, calib_loader = load_data()\n",
    "    \n",
    "    # Tạo và huấn luyện mô hình\n",
    "    model = SimpleCNN()\n",
    "    if not os.path.exists('mnist_fp32.pth'):\n",
    "        model = train_model(model, train_loader)\n",
    "        torch.save(model.state_dict(), 'mnist_fp32.pth')\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('mnist_fp32.pth'))\n",
    "    \n",
    "    # Đánh giá mô hình FP32\n",
    "    fp32_metrics = evaluate_model(model, test_loader, \"FP32\")\n",
    "    \n",
    "    # Thực hiện quantization\n",
    "    quantized_model = quantize_model(model, calib_loader)\n",
    "    \n",
    "    # Đánh giá mô hình quantized\n",
    "    int8_metrics = evaluate_model(quantized_model, test_loader, \"INT8\")\n",
    "    \n",
    "    # So sánh kích thước mô hình\n",
    "    compare_model_sizes(model, quantized_model)\n",
    "    \n",
    "    # Hiển thị kết quả so sánh\n",
    "    plot_comparison(fp32_metrics, int8_metrics)\n",
    "    \n",
    "    # Lưu mô hình quantized\n",
    "    torch.save(quantized_model.state_dict(), 'mnist_int8.pth')\n",
    "    \n",
    "    print(\"Pipeline quantization hoàn tất!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc07ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu118\n",
      "Using device: cuda:0\n",
      "Checking quantization support...\n",
      "Quantization is supported. Available engines: ['none', 'onednn', 'x86', 'fbgemm']\n",
      "Loading MNIST dataset...\n",
      "Dataset loaded successfully. Train: 48000, Val: 12000, Test: 10000\n",
      "Loading pre-trained model from mnist_fp32.pth...\n",
      "Evaluating FP32 model...\n",
      "Model is on cuda:0\n",
      "Model FP32:\n",
      "Accuracy: 98.92%\n",
      "Inference time: 5.0005 seconds\n",
      "\n",
      "===== ATTEMPTING STATIC QUANTIZATION =====\n",
      "Creating new static quantized model...\n",
      "Preparing model for static quantization...\n",
      "Copying weights to quantizable model...\n",
      "Weight copying completed. Setting model to eval mode...\n",
      "Fusing modules...\n",
      "Using quantization backend: fbgemm\n",
      "Quantization config: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Preparing model for calibration...\n",
      "Running calibration...\n",
      "Converting to fully quantized model...\n",
      "Static quantization completed successfully!\n",
      "Static quantized model saved to mnist_static_int8.pth\n",
      "Evaluating INT8 (Static) model...\n",
      "Error evaluating model: \n",
      "Comparing model sizes...\n",
      "FP32 Model Size: 1.69 MB\n",
      "INT8 (Static) Model Size: 0.43 MB\n",
      "Compression ratio: 3.89x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\Linhtinh\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. Define models with clear separation between quantizable and regular versions\n",
    "class QuantizableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizableCNN, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "    def fuse_model(self):\n",
    "        # Safer fusion approach\n",
    "        for module_name, module in self.named_children():\n",
    "            if module_name == 'quant' or module_name == 'dequant':\n",
    "                continue\n",
    "            if type(module) == nn.Sequential:\n",
    "                torch.quantization.fuse_modules(module, ['0', '1'], inplace=True)\n",
    "        \n",
    "        # Fuse conv-bn-relu\n",
    "        torch.quantization.fuse_modules(\n",
    "            self, \n",
    "            [['conv1', 'relu1'], \n",
    "             ['conv2', 'relu2'], \n",
    "             ['fc1', 'relu3']], \n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "\n",
    "# Regular model for training\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 2. Improved data loading with robust error handling\n",
    "def load_data(batch_size=64):\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load dataset with proper error handling\n",
    "        train_full = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "        \n",
    "        # Split dataset into train and validation\n",
    "        train_size = int(0.8 * len(train_full))\n",
    "        val_size = len(train_full) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            train_full, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        # Create DataLoaders with appropriate batch sizes\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "        \n",
    "        # Small dataset for calibration - keep it small and manageable\n",
    "        calib_dataset = torch.utils.data.Subset(\n",
    "            train_dataset, \n",
    "            indices=torch.randperm(len(train_dataset))[:100].tolist()\n",
    "        )\n",
    "        calib_loader = torch.utils.data.DataLoader(\n",
    "            calib_dataset,\n",
    "            batch_size=10,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded successfully. Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "        return train_loader, val_loader, test_loader, calib_loader\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# 3. Train model with early stopping and robust checkpointing\n",
    "def train_model(model, train_loader, val_loader, epochs=10, patience=3, device='cpu', checkpoint_path='checkpoint.pth'):\n",
    "    print(f\"Training model on {device}...\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            train_total_loss = 0.0\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                # Move tensors to device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                train_total_loss += loss.item()\n",
    "                \n",
    "                if i % 100 == 99:\n",
    "                    print(f'[{epoch + 1}, {i + 1}] train loss: {running_loss / 100:.3f}')\n",
    "                    running_loss = 0.0\n",
    "            \n",
    "            avg_train_loss = train_total_loss / len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "            \n",
    "            # Save history\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_accuracy)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.3f}, Val Loss: {val_loss:.3f}, Val Acc: {val_accuracy:.2f}%')\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                counter = 0\n",
    "                \n",
    "                # Save best model checkpoint\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': best_model_state,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': best_val_loss,\n",
    "                    'val_accuracy': val_accuracy\n",
    "                }, checkpoint_path)\n",
    "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "                \n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f'Early stopping triggered after epoch {epoch + 1}')\n",
    "                    break\n",
    "        \n",
    "        # Restore best model\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Plot training history\n",
    "        plot_training_history(history)\n",
    "        \n",
    "        print('Training completed successfully!')\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        # Try to restore from checkpoint if available\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            print(f\"Attempting to load model from checkpoint {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Restored model from checkpoint (epoch {checkpoint['epoch']})\")\n",
    "        return model\n",
    "\n",
    "\n",
    "# 4. Evaluate model with better error handling\n",
    "def evaluate_model(model, test_loader, name=\"FP32\", device='cpu'):\n",
    "    try:\n",
    "        print(f\"Evaluating {name} model...\")\n",
    "        model.eval()\n",
    "        \n",
    "        # If using quantized model, ensure it's on CPU\n",
    "        if 'INT8' in name and next(model.parameters()).device.type != 'cpu':\n",
    "            print(f\"Moving {name} model to CPU for evaluation (quantization requires CPU)\")\n",
    "            model = model.cpu()\n",
    "        \n",
    "        # Start timing\n",
    "        model_device = next(model.parameters()).device\n",
    "        print(f\"Model is on {model_device}\")\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Batch processing with timing\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # Match device with model\n",
    "                inputs = inputs.to(model_device)\n",
    "                labels = labels.to(model_device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        print(f\"Model {name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "        \n",
    "        return accuracy, inference_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model: {e}\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "\n",
    "# 5. Improved static quantization\n",
    "def static_quantize_model(trained_model, calib_loader):\n",
    "    print(\"Preparing model for static quantization...\")\n",
    "    \n",
    "    # Always ensure the model is on CPU for quantization\n",
    "    trained_model.cpu()\n",
    "    \n",
    "    # Create a new quantizable model instance\n",
    "    quantizable_model = QuantizableCNN()\n",
    "    \n",
    "    try:\n",
    "        # Copy weights with proper error handling\n",
    "        print(\"Copying weights to quantizable model...\")\n",
    "        for q_name, q_param in quantizable_model.named_parameters():\n",
    "            if q_name.endswith(('.weight', '.bias')):\n",
    "                # Extract module and param name\n",
    "                param_parts = q_name.split('.')\n",
    "                module_name = param_parts[0]  # e.g., 'conv1', 'fc1'\n",
    "                param_type = param_parts[-1]  # 'weight' or 'bias'\n",
    "                \n",
    "                # Construct source parameter name\n",
    "                source_name = f\"{module_name}.{param_type}\"\n",
    "                \n",
    "                # Copy parameter if found in source model\n",
    "                if source_name in dict(trained_model.named_parameters()):\n",
    "                    source_param = dict(trained_model.named_parameters())[source_name]\n",
    "                    with torch.no_grad():\n",
    "                        q_param.copy_(source_param)\n",
    "        \n",
    "        # Verify parameter copying\n",
    "        print(\"Weight copying completed. Setting model to eval mode...\")\n",
    "        quantizable_model.eval()\n",
    "        \n",
    "        # Fuse modules\n",
    "        print(\"Fusing modules...\")\n",
    "        quantizable_model.fuse_model()\n",
    "        \n",
    "        # Get supported quantization backend\n",
    "        backend = 'fbgemm'  # Default for x86\n",
    "        if torch.backends.quantized.supported_engines:\n",
    "            # Check for supported backends\n",
    "            if 'fbgemm' in torch.backends.quantized.supported_engines:\n",
    "                backend = 'fbgemm'  # Better for server (x86)\n",
    "            elif 'qnnpack' in torch.backends.quantized.supported_engines:\n",
    "                backend = 'qnnpack'  # Better for mobile (ARM)\n",
    "            \n",
    "        print(f\"Using quantization backend: {backend}\")\n",
    "        torch.backends.quantized.engine = backend\n",
    "        \n",
    "        # Set quantization configuration\n",
    "        quantizable_model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "        print(f\"Quantization config: {quantizable_model.qconfig}\")\n",
    "        \n",
    "        # Prepare model for calibration\n",
    "        print(\"Preparing model for calibration...\")\n",
    "        prepared_model = torch.quantization.prepare(quantizable_model)\n",
    "        \n",
    "        # Run calibration\n",
    "        print(\"Running calibration...\")\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, _) in enumerate(calib_loader):\n",
    "                prepared_model(inputs)\n",
    "                if i >= 9:  # Just use 10 batches for calibration\n",
    "                    break\n",
    "        \n",
    "        # Convert to quantized model\n",
    "        print(\"Converting to fully quantized model...\")\n",
    "        quantized_model = torch.quantization.convert(prepared_model)\n",
    "        \n",
    "        print(\"Static quantization completed successfully!\")\n",
    "        return quantized_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during static quantization: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# 6. Improved dynamic quantization\n",
    "def dynamic_quantize_model(model):\n",
    "    print(\"Preparing for dynamic quantization...\")\n",
    "    # Ensure model is on CPU\n",
    "    model.cpu().eval()\n",
    "    \n",
    "    try:\n",
    "        # Dynamic quantization focuses on weights of specific layers\n",
    "        print(\"Applying dynamic quantization to linear layers...\")\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            model,  # model to quantize\n",
    "            {nn.Linear},  # a set of layers to dynamically quantize\n",
    "            dtype=torch.qint8  # target quantization dtype\n",
    "        )\n",
    "        \n",
    "        print(\"Dynamic quantization completed successfully!\")\n",
    "        return quantized_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during dynamic quantization: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# 7. Compare model sizes with better visualization\n",
    "def compare_model_sizes(fp32_model, int8_model, fp32_name=\"FP32\", int8_name=\"INT8\"):\n",
    "    print(\"Comparing model sizes...\")\n",
    "    \n",
    "    def get_model_size(model, filename=\"temp_model.pth\"):\n",
    "        try:\n",
    "            # Clean up previous temp file if it exists\n",
    "            if os.path.exists(filename):\n",
    "                os.remove(filename)\n",
    "                \n",
    "            torch.save(model.state_dict(), filename)\n",
    "            size = os.path.getsize(filename) / 1e6  # MB\n",
    "            os.remove(filename)\n",
    "            return size\n",
    "        except Exception as e:\n",
    "            print(f\"Error measuring model size: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    fp32_size = get_model_size(fp32_model, \"temp_fp32.pth\")\n",
    "    int8_size = get_model_size(int8_model, \"temp_int8.pth\")\n",
    "    \n",
    "    compression_ratio = fp32_size / int8_size if int8_size > 0 else 0\n",
    "    \n",
    "    print(f\"{fp32_name} Model Size: {fp32_size:.2f} MB\")\n",
    "    print(f\"{int8_name} Model Size: {int8_size:.2f} MB\")\n",
    "    print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
    "    \n",
    "    # Create a simple bar chart for model sizes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar([fp32_name, int8_name], [fp32_size, int8_size], color=['blue', 'green'])\n",
    "    plt.title('Model Size Comparison')\n",
    "    plt.ylabel('Size (MB)')\n",
    "    plt.annotate(f\"{fp32_size:.2f} MB\", \n",
    "                 xy=(0, fp32_size), \n",
    "                 xytext=(0, fp32_size + 0.1),\n",
    "                 ha='center')\n",
    "    plt.annotate(f\"{int8_size:.2f} MB\", \n",
    "                 xy=(1, int8_size), \n",
    "                 xytext=(1, int8_size + 0.1),\n",
    "                 ha='center')\n",
    "    plt.savefig('model_size_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return fp32_size, int8_size, compression_ratio\n",
    "\n",
    "\n",
    "# 8. Plot comparison with better layout\n",
    "def plot_comparison(fp32_metrics, int8_metrics, int8_name=\"INT8\"):\n",
    "    labels = ['FP32', int8_name]\n",
    "    accuracy = [fp32_metrics[0], int8_metrics[0]]\n",
    "    inference_time = [fp32_metrics[1], int8_metrics[1]]\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    bars1 = ax1.bar(labels, accuracy, color=['blue', 'green'])\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_title('Model Accuracy Comparison')\n",
    "    ax1.set_ylim([min(accuracy) - 5 if min(accuracy) > 5 else 0, 100])\n",
    "    \n",
    "    # Add value labels on accuracy bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.2f}%',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # Plot inference time\n",
    "    bars2 = ax2.bar(labels, inference_time, color=['blue', 'green'])\n",
    "    ax2.set_ylabel('Inference Time (s)')\n",
    "    ax2.set_title('Inference Time Comparison')\n",
    "    \n",
    "    # Add value labels on time bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.annotate(f'{height:.4f}s',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # Add a speedup indicator on the inference time chart\n",
    "    if inference_time[0] > 0 and inference_time[1] > 0:\n",
    "        speedup = inference_time[0] / inference_time[1]\n",
    "        ax2.text(0.5, 0.5, f'{speedup:.2f}x speedup',\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 transform=ax2.transAxes,\n",
    "                 bbox=dict(facecolor='white', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('performance_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# 9. Main function with improved error handling and fallback options\n",
    "def main():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check device availability\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Check quantization support\n",
    "    print(\"Checking quantization support...\")\n",
    "    if hasattr(torch, 'quantization'):\n",
    "        print(f\"Quantization is supported. Available engines: {torch.backends.quantized.supported_engines}\")\n",
    "    else:\n",
    "        print(\"WARNING: This PyTorch version doesn't properly support quantization!\")\n",
    "    \n",
    "    # Set paths\n",
    "    fp32_model_path = 'mnist_fp32.pth'\n",
    "    static_int8_model_path = 'mnist_static_int8.pth'\n",
    "    dynamic_int8_model_path = 'mnist_dynamic_int8.pth'\n",
    "    checkpoint_path = 'mnist_training_checkpoint.pth'\n",
    "    \n",
    "    try:\n",
    "        # 1. Load and prepare data\n",
    "        train_loader, val_loader, test_loader, calib_loader = load_data(batch_size=64)\n",
    "        \n",
    "        # 2. Create and train/load FP32 model\n",
    "        fp32_model = SimpleCNN()\n",
    "        \n",
    "        if os.path.exists(fp32_model_path):\n",
    "            print(f\"Loading pre-trained model from {fp32_model_path}...\")\n",
    "            fp32_model.load_state_dict(torch.load(fp32_model_path, map_location=device))\n",
    "            fp32_model = fp32_model.to(device)\n",
    "        else:\n",
    "            print(f\"Training new model, will save to {fp32_model_path}...\")\n",
    "            fp32_model = train_model(\n",
    "                fp32_model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                epochs=5, \n",
    "                patience=2, \n",
    "                device=device,\n",
    "                checkpoint_path=checkpoint_path\n",
    "            )\n",
    "            # Save model to file (move to CPU first for better compatibility)\n",
    "            torch.save(fp32_model.cpu().state_dict(), fp32_model_path)\n",
    "            # Move back to original device\n",
    "            fp32_model = fp32_model.to(device)\n",
    "            print(f\"Model saved to {fp32_model_path}\")\n",
    "        \n",
    "        # 3. Evaluate FP32 model\n",
    "        fp32_metrics = evaluate_model(fp32_model, test_loader, \"FP32\", device)\n",
    "        \n",
    "        # 4. Quantization attempts - try both static and dynamic\n",
    "        \n",
    "        # Track which methods succeeded\n",
    "        static_success = False\n",
    "        dynamic_success = False\n",
    "        \n",
    "        # Try static quantization\n",
    "        try:\n",
    "            print(\"\\n===== ATTEMPTING STATIC QUANTIZATION =====\")\n",
    "            \n",
    "            if os.path.exists(static_int8_model_path):\n",
    "                print(f\"Loading pre-quantized static model from {static_int8_model_path}...\")\n",
    "                static_quantized_model = QuantizableCNN()\n",
    "                static_quantized_model.load_state_dict(torch.load(static_int8_model_path, map_location='cpu'))\n",
    "            else:\n",
    "                print(\"Creating new static quantized model...\")\n",
    "                # Copy model for quantization\n",
    "                model_for_static = SimpleCNN()\n",
    "                model_for_static.load_state_dict(fp32_model.cpu().state_dict())\n",
    "                \n",
    "                # Perform static quantization\n",
    "                static_quantized_model = static_quantize_model(model_for_static, calib_loader)\n",
    "                \n",
    "                # Save quantized model\n",
    "                torch.save(static_quantized_model.state_dict(), static_int8_model_path)\n",
    "                print(f\"Static quantized model saved to {static_int8_model_path}\")\n",
    "            \n",
    "            # Evaluate static quantized model\n",
    "            static_int8_metrics = evaluate_model(static_quantized_model, test_loader, \"INT8 (Static)\", 'cpu')\n",
    "            \n",
    "            # Compare models\n",
    "            fp32_size, static_int8_size, static_ratio = compare_model_sizes(\n",
    "                fp32_model.cpu(), static_quantized_model, \"FP32\", \"INT8 (Static)\"\n",
    "            )\n",
    "            \n",
    "            # Plot comparison\n",
    "            plot_comparison(fp32_metrics, static_int8_metrics, \"INT8 (Static)\")\n",
    "            \n",
    "            static_success = True\n",
    "            print(\"Static quantization workflow completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Static quantization workflow failed: {e}\")\n",
    "        \n",
    "        # Try dynamic quantization\n",
    "        try:\n",
    "            print(\"\\n===== ATTEMPTING DYNAMIC QUANTIZATION =====\")\n",
    "            \n",
    "            if os.path.exists(dynamic_int8_model_path):\n",
    "                print(f\"Loading pre-quantized dynamic model from {dynamic_int8_model_path}...\")\n",
    "                dynamic_quantized_model = torch.load(dynamic_int8_model_path, map_location='cpu')\n",
    "            else:\n",
    "                print(\"Creating new dynamic quantized model...\")\n",
    "                # Copy model for quantization\n",
    "                model_for_dynamic = SimpleCNN()\n",
    "                model_for_dynamic.load_state_dict(fp32_model.cpu().state_dict())\n",
    "                \n",
    "                # Perform dynamic quantization\n",
    "                dynamic_quantized_model = dynamic_quantize_model(model_for_dynamic)\n",
    "                \n",
    "                # Save quantized model\n",
    "                torch.save(dynamic_quantized_model, dynamic_int8_model_path)\n",
    "                print(f\"Dynamic quantized model saved to {dynamic_int8_model_path}\")\n",
    "            \n",
    "            # Evaluate dynamic quantized model\n",
    "            dynamic_int8_metrics = evaluate_model(dynamic_quantized_model, test_loader, \"INT8 (Dynamic)\", 'cpu')\n",
    "            \n",
    "            # Compare models\n",
    "            fp32_size, dynamic_int8_size, dynamic_ratio = compare_model_sizes(\n",
    "                fp32_model.cpu(), dynamic_quantized_model, \"FP32\", \"INT8 (Dynamic)\"\n",
    "            )\n",
    "            \n",
    "            # Plot comparison\n",
    "            plot_comparison(fp32_metrics, dynamic_int8_metrics, \"INT8 (Dynamic)\")\n",
    "            \n",
    "            dynamic_success = True\n",
    "            print(\"Dynamic quantization workflow completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Dynamic quantization workflow failed: {e}\")\n",
    "        \n",
    "        # 5. Summary\n",
    "        print(\"\\n===== QUANTIZATION RESULTS SUMMARY =====\")\n",
    "        if static_success or dynamic_success:\n",
    "            print(\"Successfully performed model quantization!\")\n",
    "            \n",
    "            if static_success and dynamic_success:\n",
    "                print(\"Both static and dynamic quantization methods were successful.\")\n",
    "                print(f\"Static quantization compression: {static_ratio:.2f}x\")\n",
    "                print(f\"Dynamic quantization compression: {dynamic_ratio:.2f}x\")\n",
    "                \n",
    "                # Compare which is better\n",
    "                if static_int8_metrics[0] > dynamic_int8_metrics[0]:\n",
    "                    print(\"Static quantization achieved better accuracy.\")\n",
    "                else:\n",
    "                    print(\"Dynamic quantization achieved better accuracy.\")\n",
    "                    \n",
    "                if static_int8_metrics[1] < dynamic_int8_metrics[1]:\n",
    "                    print(\"Static quantization achieved faster inference.\")\n",
    "                else:\n",
    "                    print(\"Dynamic quantization achieved faster inference.\")\n",
    "                    \n",
    "            elif static_success:\n",
    "                print(\"Only static quantization was successful.\")\n",
    "            else:\n",
    "                print(\"Only dynamic quantization was successful.\")\n",
    "        else:\n",
    "            print(\"Both quantization methods failed.\")\n",
    "            print(\"Possible reasons for failure:\")\n",
    "            print(\"1. PyTorch version incompatibility (version should be >= 1.8.0)\")\n",
    "            print(\"2. Missing quantization operators for your specific model/layers\")\n",
    "            print(\"3. Platform compatibility issues with quantization backends\")\n",
    "            print(\"Suggestion: Try upgrading PyTorch or simplifying your model architecture.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the main workflow: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3845e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Bắt đầu test 100 requests đồng thời\n",
      "🎯 URL: http://meizu1908.id.vn/\n",
      "👤 Username: levanminh19102003@gmail.com\n",
      "--------------------------------------------------\n",
      "Request 0: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 5: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 2: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 19: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 13: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 1: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 12: ❌ LOGIN FAIL | Response: 200 - 0.32s\n",
      "Request 7: ❌ LOGIN FAIL | Response: 200 - 0.33s\n",
      "Request 18: ❌ LOGIN FAIL | Response: 200 - 0.32s\n",
      "Request 14: ❌ LOGIN FAIL | Response: 200 - 0.32s\n",
      "Request 9: ❌ LOGIN FAIL | Response: 200 - 0.33s\n",
      "Request 23: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 20: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 6: ❌ LOGIN FAIL | Response: 200 - 0.38s\n",
      "Request 24: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 33: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 32: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 25: ❌ LOGIN FAIL | Response: 200 - 0.36s\n",
      "Request 4: ❌ LOGIN FAIL | Response: 200 - 0.39s\n",
      "Request 34: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 28: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 26: ❌ LOGIN FAIL | Response: 200 - 0.36s\n",
      "Request 22: ❌ LOGIN FAIL | Response: 200 - 0.37s\n",
      "Request 31: ❌ LOGIN FAIL | Response: 200 - 0.36s\n",
      "Request 30: ❌ LOGIN FAIL | Response: 200 - 0.36s\n",
      "Request 10: ❌ LOGIN FAIL | Response: 200 - 0.40s\n",
      "Request 11: ❌ LOGIN FAIL | Response: 200 - 0.40s\n",
      "Request 15: ❌ LOGIN FAIL | Response: 200 - 0.39s\n",
      "Request 17: ❌ LOGIN FAIL | Response: 200 - 0.39s\n",
      "Request 27: ❌ LOGIN FAIL | Response: 200 - 0.38s\n",
      "Request 21: ❌ LOGIN FAIL | Response: 200 - 0.39s\n",
      "Request 3: ❌ LOGIN FAIL | Response: 200 - 0.42s\n",
      "Request 29: ❌ LOGIN FAIL | Response: 200 - 0.38s\n",
      "Request 48: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 47: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 8: ❌ LOGIN FAIL | Response: 200 - 0.42s\n",
      "Request 16: ❌ LOGIN FAIL | Response: 200 - 0.40s\n",
      "Request 42: ❌ LOGIN FAIL | Response: 200 - 0.36s\n",
      "Request 35: ❌ LOGIN FAIL | Response: 200 - 0.37s\n",
      "Request 37: ❌ LOGIN FAIL | Response: 200 - 0.38s\n",
      "Request 38: ❌ LOGIN FAIL | Response: 200 - 0.39s\n",
      "Request 46: ❌ LOGIN FAIL | Response: 200 - 0.38s\n",
      "Request 36: ❌ LOGIN FAIL | Response: 200 - 0.40s\n",
      "Request 45: ❌ LOGIN FAIL | Response: 200 - 0.38s\n",
      "Request 39: ❌ LOGIN FAIL | Response: 200 - 0.39s\n",
      "Request 43: ❌ LOGIN FAIL | Response: 200 - 0.39s\n",
      "Request 49: ❌ LOGIN FAIL | Response: 200 - 0.38s\n",
      "Request 41: ❌ LOGIN FAIL | Response: 200 - 0.40s\n",
      "Request 44: ❌ LOGIN FAIL | Response: 200 - 0.39s\n",
      "Request 40: ❌ LOGIN FAIL | Response: 200 - 0.41s\n",
      "Request 50: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 51: ❌ LOGIN FAIL | Response: 200 - 0.28s\n",
      "Request 53: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 54: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 55: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 60: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 52: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 59: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 57: ❌ LOGIN FAIL | Response: 200 - 0.32s\n",
      "Request 56: ❌ LOGIN FAIL | Response: 200 - 0.33s\n",
      "Request 61: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 58: ❌ LOGIN FAIL | Response: 200 - 0.33s\n",
      "Request 76: ❌ LOGIN FAIL | Response: 200 - 0.28s\n",
      "Request 75: ❌ LOGIN FAIL | Response: 200 - 0.28s\n",
      "Request 67: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 62: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 78: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 87: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 83: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 79: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 69: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 73: ❌ LOGIN FAIL | Response: 200 - 0.33s\n",
      "Request 64: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 77: ❌ LOGIN FAIL | Response: 200 - 0.32s\n",
      "Request 66: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 71: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 86: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 63: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 72: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 88: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 93: ❌ LOGIN FAIL | Response: 200 - 0.28s\n",
      "Request 82: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 97: ❌ LOGIN FAIL | Response: 200 - 0.27s\n",
      "Request 90: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 98: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 85: ❌ LOGIN FAIL | Response: 200 - 0.32s\n",
      "Request 92: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 68: ❌ LOGIN FAIL | Response: 200 - 0.36s\n",
      "Request 74: ❌ LOGIN FAIL | Response: 200 - 0.35s\n",
      "Request 89: ❌ LOGIN FAIL | Response: 200 - 0.33s\n",
      "Request 99: ❌ LOGIN FAIL | Response: 200 - 0.29s\n",
      "Request 65: ❌ LOGIN FAIL | Response: 200 - 0.37s\n",
      "Request 91: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 96: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 81: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 70: ❌ LOGIN FAIL | Response: 200 - 0.37s\n",
      "Request 84: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "Request 95: ❌ LOGIN FAIL | Response: 200 - 0.30s\n",
      "Request 94: ❌ LOGIN FAIL | Response: 200 - 0.31s\n",
      "Request 80: ❌ LOGIN FAIL | Response: 200 - 0.34s\n",
      "\n",
      "==================================================\n",
      "📊 KẾT QUẢ\n",
      "==================================================\n",
      "⏱️  Tổng thời gian: 0.77s\n",
      "📊 Tổng requests: 100\n",
      "🔐 Đăng nhập thành công: 0\n",
      "🚫 Đăng nhập thất bại: 100\n",
      "✅ Requests thành công: 0\n",
      "❌ Requests thất bại: 100\n",
      "\n",
      "⚠️  CẢNH BÁO: 100 lần đăng nhập thất bại!\n",
      "Kiểm tra lại:\n",
      "- URL login: http://meizu1908.id.vn/api/auth/login\n",
      "- Username: levanminh19102003@gmail.com\n",
      "- Mật khẩu có đúng không?\n",
      "- Trang có dùng React/MUI form không? (có thể cần API endpoint khác)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple Load Test Script - 100 concurrent requests với login\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Cấu hình\n",
    "URL = \"http://meizu1908.id.vn/\"\n",
    "LOGIN_URL = \"http://meizu1908.id.vn/api/auth/login\"  # Thử API endpoint\n",
    "LOGIN_URL_FALLBACK = \"http://meizu1908.id.vn/login\"  # Fallback nếu không có API\n",
    "USERNAME = \"levanminh19102003@gmail.com\"\n",
    "PASSWORD = \"123456\"\n",
    "NUM_REQUESTS = 100\n",
    "\n",
    "# Biến lưu kết quả\n",
    "results = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def make_request(request_id):\n",
    "    \"\"\"Thực hiện 1 request với đăng nhập\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Tạo session để giữ cookie\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # Lấy trang chính trước để lấy cookies/tokens\n",
    "        session.get(URL, timeout=10)\n",
    "        \n",
    "        # Thử đăng nhập với JSON API trước\n",
    "        login_data = {\n",
    "            'username': USERNAME,\n",
    "            'password': PASSWORD\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        login_success = False\n",
    "        \n",
    "        # Thử API endpoint trước\n",
    "        try:\n",
    "            login_response = session.post(\n",
    "                LOGIN_URL, \n",
    "                json=login_data,  # Gửi JSON\n",
    "                headers=headers,\n",
    "                timeout=10\n",
    "            )\n",
    "            \n",
    "            if login_response.status_code == 200:\n",
    "                # Kiểm tra response JSON\n",
    "                try:\n",
    "                    json_response = login_response.json()\n",
    "                    if json_response.get('success') or json_response.get('token') or json_response.get('user'):\n",
    "                        login_success = True\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Nếu API fail, thử form thông thường\n",
    "            try:\n",
    "                login_response = session.post(\n",
    "                    LOGIN_URL_FALLBACK,\n",
    "                    data=login_data,  # Gửi form data\n",
    "                    timeout=10\n",
    "                )\n",
    "                \n",
    "                if login_response.status_code == 302:  # Redirect\n",
    "                    login_success = True\n",
    "                elif login_response.status_code == 200:\n",
    "                    if any(keyword in login_response.text.lower() \n",
    "                          for keyword in ['dashboard', 'welcome', 'logout', 'profile']):\n",
    "                        login_success = True\n",
    "                        \n",
    "            except Exception as e2:\n",
    "                pass\n",
    "        \n",
    "        # Request chính sau khi đăng nhập\n",
    "        response = session.get(URL, timeout=10)\n",
    "        \n",
    "        # Kiểm tra thêm bằng cách xem có content khác không\n",
    "        if not login_success and response.status_code == 200:\n",
    "            # Nếu có những từ này trong response thì có thể đã login\n",
    "            if any(keyword in response.text.lower() \n",
    "                  for keyword in ['dashboard', 'profile', 'logout', 'welcome']):\n",
    "                login_success = True\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # Lưu kết quả\n",
    "        with lock:\n",
    "            results.append({\n",
    "                'id': request_id,\n",
    "                'login_success': login_success,\n",
    "                'status': response.status_code,\n",
    "                'time': response_time,\n",
    "                'success': response.status_code == 200 and login_success\n",
    "            })\n",
    "        \n",
    "        login_status = \"✅ LOGIN OK\" if login_success else \"❌ LOGIN FAIL\"\n",
    "        print(f\"Request {request_id}: {login_status} | Response: {response.status_code} - {response_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        with lock:\n",
    "            results.append({\n",
    "                'id': request_id,\n",
    "                'login_success': False,\n",
    "                'status': 0,\n",
    "                'time': response_time,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "        print(f\"Request {request_id}: ❌ ERROR - {str(e)}\")\n",
    "\n",
    "def run_load_test():\n",
    "    \"\"\"Chạy load test\"\"\"\n",
    "    print(f\"🚀 Bắt đầu test {NUM_REQUESTS} requests đồng thời\")\n",
    "    print(f\"🎯 URL: {URL}\")\n",
    "    print(f\"👤 Username: {USERNAME}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Chạy 100 requests đồng thời\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        futures = [executor.submit(make_request, i) for i in range(NUM_REQUESTS)]\n",
    "        \n",
    "        # Đợi tất cả hoàn thành\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 KẾT QUẢ\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    successful = [r for r in results if r['success']]\n",
    "    failed = [r for r in results if not r['success']]\n",
    "    login_success = [r for r in results if r.get('login_success', False)]\n",
    "    login_failed = [r for r in results if not r.get('login_success', True)]\n",
    "    \n",
    "    print(f\"⏱️  Tổng thời gian: {total_time:.2f}s\")\n",
    "    print(f\"📊 Tổng requests: {len(results)}\")\n",
    "    print(f\"🔐 Đăng nhập thành công: {len(login_success)}\")\n",
    "    print(f\"🚫 Đăng nhập thất bại: {len(login_failed)}\")\n",
    "    print(f\"✅ Requests thành công: {len(successful)}\")\n",
    "    print(f\"❌ Requests thất bại: {len(failed)}\")\n",
    "    \n",
    "    if successful:\n",
    "        avg_time = sum(r['time'] for r in successful) / len(successful)\n",
    "        min_time = min(r['time'] for r in successful)\n",
    "        max_time = max(r['time'] for r in successful)\n",
    "        \n",
    "        print(f\"⚡ Thời gian trung bình: {avg_time:.2f}s\")\n",
    "        print(f\"🏃 Nhanh nhất: {min_time:.2f}s\")\n",
    "        print(f\"🐌 Chậm nhất: {max_time:.2f}s\")\n",
    "        print(f\"🔥 Requests/giây: {len(successful)/total_time:.2f}\")\n",
    "    \n",
    "    # Hiển thị một số lỗi login nếu có\n",
    "    if login_failed:\n",
    "        print(f\"\\n⚠️  CẢNH BÁO: {len(login_failed)} lần đăng nhập thất bại!\")\n",
    "        print(\"Kiểm tra lại:\")\n",
    "        print(f\"- URL login: {LOGIN_URL}\")\n",
    "        print(f\"- Username: {USERNAME}\")\n",
    "        print(\"- Mật khẩu có đúng không?\")\n",
    "        print(\"- Trang có dùng React/MUI form không? (có thể cần API endpoint khác)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_load_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Linhtinh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
